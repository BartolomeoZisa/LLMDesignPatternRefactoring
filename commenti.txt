State: good refactoring tests passed
Strategy: added simple factory, check it. tests passed
Factory: added simple factory to keep retrocompatibility, check it. tests passed
Adapter weather: added functionality, how can it do that? The weather api used isn't changed tests passed (?) (could add a strategy)
Decorator: how to maintain public interface ??? tests not passed
Singleton Resource manager: idiomatic solution, problem in testing singleton sequentially. tests not passed (could add factory)


WEEK 2
-----------------------------------------------------

State: simple market good refactoring, tests passed
vector: sort strategy tests passed
graph: abstract factory + adapter.  tests to make (could make strategy version?)
monster: factory method tests only on creation but passed battle could be added
Icecream: decorator tests passed
singleton: resources manager, tests passed is initialization done properly?
Adapter: calculator api tests passed, maybe too easy

graph discarded
Singleton was a bit too hard, on hold

WEEK 3
---------------------------------------------------

Making refactoring process semiautomatic, a helper function finds triples (basecode, pattern, testsforRefactored).
A prompt is generated including those and a pattern description. It can be then fed to a LLM by hand and the result is automatically
tested by creating a folder structure where pytest is run. I had to use subprocess.run to make the execution independent, else the module
import causes problems.

Adapter calculator: 
GPT very simple, always the same result, doesn't create the interfaces (is it properly using the pattern?)
Gemini, uses interfaces, textbook implementation of the pattern
Copilot, tests passed but doesn't create interfaces, also creates things which aren't code!!
--validation try adding another adapter? (problematic, the tests don't specify the interface name)
(maybe add a test that check if there's a class that extends ABC to enforce the interface creation)
Not really certain of a good way.

Decorator icecream: 
for GPT 1 failed, 2 and 3 passed the tests 
Only 3 seems a good refactoring: in 2 the dictionary is kept instead of making a new concrete strategy
Gemini: all tests passed but the names of the interfaces are different
Copilot: tests passed but the names are different, test 2 didn't pass because of added comments
--validation: code to check dictionaries in this case or (better) test adding a concrete strategy/concrete decorator

State Stall: 
GPT all tests passed, very similar results
Gemini: all tests passed
Copilot: the prompt is too long, had to delete the description, all tests passed, no added comments
--validation either add state or check for if and elifs, there should be a lot less

Strategy Vector:
GPT failed the first test due to the name of the strategy
Gemini failed all tests due to the name of the strategy (my fault because in the tests i gave a lowercase class name?)
Copilot all versions don't pass a test case since they use set_sort_strategy as a name instead of setSortStrategy
this is probably because all the tests use underscore_case instead of camelCase
--validation should check adding new strategy?

Factory method monster:
GPT all tests passed, but the methods are a bit different than expected, 
Gemini all tests passed, the solutions are similar, but different from chatgpt and my solution. 
Copilot 1 and 3 passed, 2 not passed due to ghoul, different from mine
In the tests only create_monster is called (should the testing enforce all of the methods? the others would be private)
--validation try to add new concrete classes and factories

Chatgpt does well but not with the decorator

Gemini seems to be the best one

Documentation on copilot prompting is scarce
Seems to ignore that it should only give the code as output expecially if it has a long input
It has a smaller context window. 

WEEK 4
-------------------------------------------------------------------------------------------------
save input lenght and output length in words, input+output should be <4000
in prompt infer interface from tests

TODO: change prompt
read from json (strategy to read)
using chatgptAPI (strategy to create output)
visualize results
TODO: save errors in report

-------

Copied  gang of four definition, and changed structure to plantuml representation then created a parser based on headers. 
Intent, Motivation, Structure, Participants, Collaborations, Implementation, Sample Code headers = around 2200 words

Changed prompt (for now hardcoding language)
Markdown is a big problem, it's adding backticks and "python", parsed it by removing first and last line
Is the json fine?
cleaned code by indenting properly, and solving to copy paste errors with aspell

Made plot_report util to print test results.

Calculator doesn't work because I'm importing OldCalculator, and it changes names to Calculator (sometimes it's an interface, sometimes it's not)
Icecream is sometimes not declaring stuff it's using (like cone or cup)
Monster is sometimes working, sometimes not (in one case it was putting less legs than necessary)
Vector is working by changing the names to follow underscore_case

Before it performed better.
What is the problem? 4omini? the prompt? the fact I'm using the api? the formmatted design pattern description? 
The length of the input seems likely the tokens are more than string.split() words, input+output = 4000 for some examples
checking the openai tokenizer https://platform.openai.com/tokenizer

Changed the prompt to use xml to properly discern the parts, added the line 
"Do not rename any classes, functions, or modules in a way that would break existing import statements in the tests."
This seemed to make the adapter work (gpt 8-9)
I have a feeling putting the tests after the descriptions, or the lengthy descriptions in general, could make it perform worse???

-------------------------------------------------------
For the LLMs we want to set
Temperature ðŸ—¸
Model name ðŸ—¸
Context length (In Openai it's not settable, max token is settable) ðŸ—¸
csv with parameters should be created? ðŸ—¸ (json for now)
Saving result is necessary, so a folder is needed as input ðŸ—¸



Change openai signature. ðŸ—¸ ?
Add __init__.py in tester ðŸ—¸
change naming convention in the tester ðŸ—¸

The settings should be set by command line for the CLI
These can be changed in the options settings for the tool that makes multiple executions

Best to put the file creation in the backend or frontend?

Select headers to exclude from the description
Select parts of the prompt to exclude (?)




prompt creation and response (not testing) should be handled in a class called RefactorFrontEnd, which should be called by itself by command line passing arguments code_path, refactored_tests_path, pattern_name, prompt_file_path: optional args should be parsed  for Temperature, Model name, Context length, if not present put the default 

What do I put in the frontend class?
Reloading from prompt necessary???

Example: (change tester)
python3 refactorer.py /home/bartolomeo/projects/Tesi/examples/behavioural/state/simplemarket/base_folder/base/Stall.py /home/bartolomeo/projects/Tesi/examples/behavioural/state/simplemarket/refactored_folder/test_refactored/test_Stall.py state /home/bartolomeo/projects/Tesi/refactoringsystem/prompts/promptxml.txt /home/bartolomeo/projects/Tesi/examples/behavioural/state/simplemarket/llm2
python3 tester.py /home/bartolomeo/projects/Tesi/examples/behavioural/state/simplemarket/llm2/Stall_state_gpt-4o-mini-2024-07-18_20250504_143206/refactored/Stall.py /home/bartolomeo/projects/Tesi/examples/behavioural/state/simplemarket/refactored_folder/test_refactored/test_Stall.py